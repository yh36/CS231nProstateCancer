{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import time\n",
    "import copy\n",
    "import pickle  # Log dictionary data\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # stateless functions\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "import multiprocessing\n",
    "# We must import this explicitly, it is not imported by the top-level\n",
    "# multiprocessing module.\n",
    "import multiprocessing.pool\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score,confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from datetime import datetime\n",
    "from multiprocessing import Manager\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import glob\n",
    "import cv2\n",
    "import time\n",
    "import copy\n",
    "import pickle  # Log dictionary data\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # stateless functions\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import cohen_kappa_score,confusion_matrix\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configuration in Common\n",
    "\"\"\"\n",
    "class CFG:\n",
    "    debug = False\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.float32\n",
    "    nfolds = 4\n",
    "    seed = 524\n",
    "    TRAIN = '../yi_data/panda-16x128x128-tiles-data/train/'\n",
    "    LABELS = '../data/train.csv'\n",
    "\n",
    "\"\"\"Configuration for Stage One\n",
    "\"\"\"\n",
    "class CFG1:\n",
    "    batch_size = 16    \n",
    "    epochs = 16\n",
    "    lr = 1e-4\n",
    "    model_name = 'resnet_bc'\n",
    "    num_classes = 1\n",
    "    nworkers = 1    \n",
    "    threshold = .32\n",
    "    \n",
    "\"\"\"Configuration for Stage Two\n",
    "\"\"\"\n",
    "class CFG2:\n",
    "    batch_size = 16\n",
    "    epochs = 16\n",
    "    lr = 1e-4\n",
    "    model_name = 'resnet'\n",
    "    num_classes = 3\n",
    "    nworkers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.LABELS).set_index('image_id')\n",
    "files = sorted(set([p[:32] for p in os.listdir(CFG.TRAIN)]))\n",
    "train = train.loc[files].reset_index()\n",
    "\n",
    "if CFG.debug:\n",
    "    df = train.sample(n=50, random_state=CFG.seed).copy().reset_index(drop=True)\n",
    "else:\n",
    "    df = train.copy()\n",
    "\n",
    "# Generate train/validation sets containing the same distribution of isup_grade\n",
    "splits = StratifiedKFold(n_splits=CFG.nfolds, random_state=CFG.seed, shuffle=True)\n",
    "splits = list(splits.split(df,df.isup_grade))\n",
    "# Assign split index to training samples\n",
    "folds_splits = np.zeros(len(df)).astype(np.int)\n",
    "for i in range(CFG.nfolds):\n",
    "    folds_splits[splits[i][1]] = i\n",
    "df['split'] = folds_splits\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/yasufuminakama/panda-se-resnext50-regression-baseline\n",
    "class TrainDataset(Dataset):\n",
    "    \"\"\"Prostate Cancer Biopsy Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, df, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file\n",
    "            root_dir (string): Path to the directory with all images\n",
    "            transform (callable, optional): Optional transform to be applied on an image sample\n",
    "        \"\"\"\n",
    "        # Shuffle dataframes with fixed seed; otherwise, validation set only get cancerous samples\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # https://stackoverflow.com/questions/33369832/read-multiple-images-on-a-folder-in-opencv-python\n",
    "        img_fns = [fn for fn in glob.glob(f\"{CFG.TRAIN}/{self.df['image_id'][idx]}_*.png\")]\n",
    "        # As we use cv2, the color channel is BGR. https://stackoverflow.com/questions/50963283/python-opencv-imshow-doesnt-need-convert-from-bgr-to-rgb\n",
    "        imgs = [cv2.imread(fn) for fn in img_fns]\n",
    "        # (D,W,H)\n",
    "        img = cv2.hconcat([cv2.vconcat([imgs[0], imgs[1], imgs[2], imgs[3]]),\n",
    "                           cv2.vconcat([imgs[4], imgs[5], imgs[6], imgs[7]]),\n",
    "                           cv2.vconcat([imgs[8], imgs[9], imgs[10], imgs[11]]),\n",
    "                           cv2.vconcat([imgs[12], imgs[13], imgs[14], imgs[15]])])\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        label = self.labels[idx]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase):\n",
    "    assert phase in {'train', 'val'}\n",
    "    \n",
    "    if phase == 'train':\n",
    "        return T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomVerticalFlip(),\n",
    "            T.RandomRotation(15, fill=255),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "                mean=[0.8776, 0.8186, 0.9090],\n",
    "                std=[0.1659, 0.2507, 0.1357],\n",
    "            ),\n",
    "        ])\n",
    "    else:\n",
    "        return T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "                mean=[0.8776, 0.8186, 0.9090],\n",
    "                std=[0.1659, 0.2507, 0.1357],\n",
    "            ),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "transform = get_transforms(phase='train')\n",
    "train_dataset = TrainDataset(df.reset_index(drop=True),\n",
    "                             df.reset_index(drop=True)['isup_grade'],\n",
    "                             transform = get_transforms(phase='train'))\n",
    "img, label = train_dataset[0]\n",
    "#tiles = map(transform, tiles)\n",
    "#print(list(tiles))\n",
    "#print(label)\n",
    "#print(tiles.shape)\n",
    "#print(type(tiles[0]))\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "#plt.show()\n",
    "#plt.imshow(tiles[0].permute(1,2,0))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fold idx as validation set\n",
    "def data_loader_stage1(fold_idx):\n",
    "    train_idx = df[df['split'] != fold_idx].index\n",
    "    val_idx = df[df['split'] == fold_idx].index\n",
    "\n",
    "    train_dataset = TrainDataset(df.loc[train_idx].reset_index(drop=True),\n",
    "                                 df.loc[train_idx].reset_index(drop=True)['isup_grade'],\n",
    "                                 transform = get_transforms(phase='train'))\n",
    "    val_dataset = TrainDataset(df.loc[val_idx].reset_index(drop=True),\n",
    "                               df.loc[val_idx].reset_index(drop=True)['isup_grade'],\n",
    "                               transform = get_transforms(phase='val'))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG1.batch_size, shuffle=True, num_workers=CFG1.nworkers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CFG1.batch_size, shuffle=False, num_workers=CFG1.nworkers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def data_loader_stage2(fold_idx):\n",
    "    train_idx = df[(df['split'] != fold_idx) & (df['isup_grade'] > 2)].index\n",
    "    val_idx = df[(df['split'] == fold_idx) & (df['isup_grade'] > 2)].index\n",
    "\n",
    "    train_dataset = TrainDataset(df.loc[train_idx].reset_index(drop=True),\n",
    "                                 df.loc[train_idx].reset_index(drop=True)['isup_grade']-3,\n",
    "                                 transform = get_transforms(phase='train'))\n",
    "    val_dataset = TrainDataset(df.loc[val_idx].reset_index(drop=True),\n",
    "                               df.loc[val_idx].reset_index(drop=True)['isup_grade']-3,\n",
    "                               transform = get_transforms(phase='val'))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG2.batch_size, shuffle=True, num_workers=CFG2.nworkers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CFG2.batch_size, shuffle=False, num_workers=CFG2.nworkers)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(stage, model, fold, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    # Send the model to GPU/CPU\n",
    "    model = model.to(device=CFG.device)\n",
    "    \n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            preds, targets = [], []\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()   # Set model to training phase\n",
    "            else:\n",
    "                model.eval()    # Set model to evaluate phase\n",
    "            \n",
    "            avg_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            print(' ', end='', flush=True)  # To workaround tqdm issue in multiprocess\n",
    "            for inputs, labels in tqdm(dataloaders[phase],\n",
    "                                       desc='[{}-{}] {}/{}({:5s})'.format(stage, fold, epoch+1,num_epochs,phase)):\n",
    "                inputs = inputs.to(device=CFG.device, dtype=CFG.dtype)\n",
    "                if stage == 1:\n",
    "                    labels = torch.where(labels<3, torch.tensor(0), torch.tensor(1)).to(device=CFG.device, dtype=CFG.dtype)\n",
    "                elif stage == 2:\n",
    "                    labels = labels.to(device=CFG.device, dtype=torch.long)\n",
    "                else:\n",
    "                    assert False\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward, track history if only in training\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    #print(outputs.shape, labels.shape)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    #print(outputs)\n",
    "                    if stage == 1:\n",
    "                        pred = torch.where(outputs<CFG1.threshold,\n",
    "                                           torch.tensor(0).to(device=CFG.device),\n",
    "                                           torch.tensor(1).to(device=CFG.device)).to(device=CFG.device, dtype=torch.long)\n",
    "                        labels = labels.to(dtype=torch.long)\n",
    "                    #print(pred)\n",
    "                    #print(labels)\n",
    "                    elif stage == 2:\n",
    "                        pred = torch.argmax(outputs, 1)\n",
    "                        print(type(pred))\n",
    "                        #print(pred.shape)\n",
    "                    else:\n",
    "                        assert False\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # Statistics\n",
    "                avg_loss += loss.item()*(inputs.size(0)/len(dataloaders[phase].dataset))  # len(dataloaders[phase].dataset) not len(dataloaders[phase])\n",
    "                running_corrects += torch.sum(pred == labels)\n",
    "                #print('correts: ', torch.sum(pred == labels))\n",
    "                preds.append(pred)\n",
    "                targets.append(labels)\n",
    "            \n",
    "            # End of epoch\n",
    "            with torch.no_grad():\n",
    "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "                if phase == 'val':\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "                    # deep copy the model\n",
    "                    if epoch_acc > best_acc:\n",
    "                        best_acc = epoch_acc\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    # Apply lr_scheduler\n",
    "                    if scheduler is not None:\n",
    "                        scheduler.step(avg_loss)\n",
    "                else:\n",
    "                    train_acc_history.append(epoch_acc)\n",
    "                    loss_history.append(avg_loss)\n",
    "                print('[{}-{}] {} Loss: {:4f} Acc: {:4f}'.format(stage, fold, phase, avg_loss, epoch_acc))\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('[{}-{}] Training complete in {:.0f}m {:0f}s'.format(stage, fold, time_elapsed//60, time_elapsed%60))\n",
    "    print('[{}-{}] Best val Acc: {:4f}'.format(stage, fold, best_acc))\n",
    "    print()\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        feature_extract\n",
    "            True - fine tunning\n",
    "            False - fix the model\n",
    "    \"\"\"\n",
    "    model_ft = None\n",
    "    \n",
    "    if model_name == 'alexnet':\n",
    "        \"\"\"AlexNet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == 'resnet':\n",
    "        \"\"\"Resnet\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == 'resnet_bc':\n",
    "        \"\"\"Resnet\n",
    "        \"\"\"\n",
    "        res_model = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(res_model, feature_extract)\n",
    "        num_ftrs = res_model.fc.in_features\n",
    "        res_model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        model_ft = nn.Sequential(\n",
    "                    res_model,\n",
    "                    nn.Sigmoid(),  # Squeeze output within [0,1]\n",
    "        )\n",
    "    return model_ft\n",
    "\n",
    "\n",
    "def train_stage1_layer(fold):\n",
    "    model_ft = initialize_model(CFG1.model_name, CFG1.num_classes, use_pretrained=False)\n",
    "\n",
    "    optimizer = optim.Adam(model_ft.parameters(),\n",
    "                           lr=CFG1.lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True, eps=1e-06)\n",
    "    loader_train, loader_val = data_loader_stage1(fold)\n",
    "    return train_model(1, model_ft, fold,\n",
    "                       {'train': loader_train, 'val': loader_val},\n",
    "                       nn.BCELoss(), optimizer, scheduler, CFG1.epochs)\n",
    "\n",
    "def train_stage2_layer(fold):\n",
    "    model_ft = initialize_model(CFG2.model_name, CFG2.num_classes, use_pretrained=False)\n",
    "    optimizer = optim.Adam(model_ft.parameters(),\n",
    "                           lr=CFG2.lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True, eps=1e-06)\n",
    "    loader_train, loader_val = data_loader_stage2(fold)\n",
    "    return train_model(2, model_ft, fold,\n",
    "                      {'train': loader_train, 'val': loader_val},\n",
    "                      F.cross_entropy, optimizer, scheduler, CFG2.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoDaemonProcess(multiprocessing.Process):\n",
    "    # make 'daemon' attribute always return False\n",
    "    def _get_daemon(self):\n",
    "        return False\n",
    "    def _set_daemon(self, value):\n",
    "        pass\n",
    "    daemon = property(_get_daemon, _set_daemon)\n",
    "\n",
    "# We sub-class multiprocessing.pool.Pool instead of multiprocessing.Pool\n",
    "# because the latter is only a wrapper function, not a proper class.\n",
    "class MyPool(multiprocessing.pool.Pool):\n",
    "    Process = NoDaemonProcess\n",
    "\n",
    "def progressor(fold):\n",
    "    #print(f'stage{stage} fold{fold}')\n",
    "    best_model_1, loss_history_1, train_acc_history_1, val_acc_history_1 = train_stage1_layer(fold)\n",
    "    best_model_2, loss_history_2, train_acc_history_2, val_acc_history_2 = train_stage2_layer(fold)\n",
    "    return {f'stage1_model_{fold}': best_model_1.to('cpu'),  # Don't save model as cuda\n",
    "            f'stage1_loss_history_{fold}': loss_history_1,\n",
    "            f'stage1_train_acc_history_{fold}': train_acc_history_1,\n",
    "            f'stage1_val_acc_history_{fold}': val_acc_history_1,\n",
    "            f'stage2_model_{fold}': best_model_2.to('cpu'),  # Don't save model as cuda\n",
    "            f'stage2_loss_history_{fold}': loss_history_2,\n",
    "            f'stage2_train_acc_history_{fold}': train_acc_history_2,\n",
    "            f'stage2_val_acc_history_{fold}': val_acc_history_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {'batch_size_1': CFG1.batch_size,\n",
    "            'batch_size_2': CFG2.batch_size,\n",
    "            'epochs_1': CFG1.epochs,\n",
    "            'epochs_2': CFG2.epochs,\n",
    "            'learning_rate_1': CFG1.lr,\n",
    "            'learning_rate_2': CFG2.lr,\n",
    "            'model_1': CFG1.model_name,\n",
    "            'model_2': CFG2.model_name,\n",
    "            'nworkers_1': CFG1.nworkers,\n",
    "            'nworkers_2': CFG2.nworkers,\n",
    "            'nfolds': CFG.nfolds,\n",
    "            'random_seed': CFG.seed}\n",
    "\n",
    "result_list = list(MyPool(CFG.nfolds).map(progressor, range(CFG.nfolds)))\n",
    "\n",
    "# Accumulate result from each process\n",
    "for result in result_list:\n",
    "    log_dict.update(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "## Analyze Stage One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, scores = [], []\n",
    "for fold in range(CFG.nfolds):\n",
    "    model_fd = log_dict[f'stage1_model_{fold}'].to(device=CFG.device, dtype=CFG.dtype)\n",
    "    _, loader_val = data_loader_stage1(fold)\n",
    "    for inputs, labels in tqdm(loader_val):\n",
    "        inputs = inputs.to(device=CFG.device, dtype=CFG.dtype)\n",
    "        labels = torch.where(labels<3, torch.tensor(0), torch.tensor(1)).to(device=CFG.device, dtype=CFG.dtype)\n",
    "                \n",
    "        # Forward, track history if only in training\n",
    "        with torch.no_grad():\n",
    "            outputs = model_fd(inputs).squeeze()\n",
    "        targets.append(labels)\n",
    "        scores.append(outputs)\n",
    "\n",
    "t1 = torch.cat(targets).cpu()\n",
    "s1 = torch.cat(scores).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "fpr, tpr, threshold = metrics.roc_curve(t1, s1.view(-1))\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "log_dict['fpr'] = fpr\n",
    "log_dict['tpr'] = tpr\n",
    "log_dict['threshold'] = threshold\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tn, fp, fn, tp = conf_mat.ravel()\n",
    "print(f'sensitivity: {tp/(tp+fn)}')\n",
    "print(f'specificity: {tn/(tn+fp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Stage Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, preds, targets = [], [], []\n",
    "for fold in range(CFG.nfolds):\n",
    "    model_fd1 = log_dict[f'stage1_model_{fold}'].to(device=CFG.device, dtype=CFG.dtype)\n",
    "    model_fd2 = log_dict[f'stage2_model_{fold}'].to(device=CFG.device, dtype=CFG.dtype)\n",
    "    _, loader_val = data_loader_stage2(fold)\n",
    "    for inputs, labels in tqdm(loader_val):\n",
    "        inputs = inputs.to(device=CFG.device, dtype=CFG.dtype)\n",
    "                \n",
    "        # Forward, track history if only in training\n",
    "        with torch.no_grad():\n",
    "            probs = model_fd1(inputs).squeeze()\n",
    "            outputs = model_fd2(inputs)\n",
    "            mask = torch.where(probs<CFG1.threshold,\n",
    "                               torch.tensor(0).to(device=CFG.device),\n",
    "                               torch.tensor(1).to(device=CFG.device))\n",
    "            pred = torch.argmax(outputs, 1)\n",
    "        masks.append(mask)\n",
    "        targets.append(labels)\n",
    "        preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = torch.cat(preds).cpu()\n",
    "t2 = torch.cat(targets).cpu()\n",
    "m2 = torch.cat(masks).cpu()\n",
    "\n",
    "kappa = cohen_kappa_score(t2[m2==1], p2[m2==1], weights='quadratic')\n",
    "print(f'Kappa: {kappa}')\n",
    "conf_mat = confusion_matrix(t2[m2==1], p2[m2==1])\n",
    "#plt.matshow()\n",
    "plt.figure(figsize=(14,7))\n",
    "isup_labels = list(range(3,6))\n",
    "sn.heatmap(conf_mat, annot=True, xticklabels=isup_labels, yticklabels=isup_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = f'{CFG1.model_name}-{CFG2.model_name}_{datetime.now().strftime(\"%m_%d_%Y_%H_%M\")}.pkl'\n",
    "with open(log_file, 'wb') as pkl_file:\n",
    "    pickle.dump(log_dict, pkl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
