{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PyTorch Dataset: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # stateless functions\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import torchvision.datasets as dset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "image_dir = '../data/train_images'\n",
    "NUM_TRAIN = 6800 #8492\n",
    "\n",
    "USE_GPU = False\n",
    "dtype = torch.float32   # use float throughout the training\n",
    "print_every = 10\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized Dataset\n",
    "class ProstateCancerDataset(Dataset):\n",
    "    \"\"\"Prostate Cancer Biopsy Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file\n",
    "            root_dir (string): Path to the directory with all images\n",
    "            transform (callable, optional): Optional transform to be applied on an image sample\n",
    "        \"\"\"\n",
    "        # Shuffle dataframes with fixed seed\n",
    "        self.cancer_df = pd.read_csv(csv_file).sample(frac=1, random_state=1)\n",
    "        # Use DataLoader to shuffer data\n",
    "        # self.cancer_df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cancer_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, f'{self.cancer_df.iloc[idx, 0]}.tiff')\n",
    "        # (D,W,H)\n",
    "        #img = io.imread(img_path)\n",
    "        img = io.MultiImage(img_path) # conserve_memory=True  Turn off to improve performance\n",
    "        isup = self.cancer_df.iloc[idx, 2]\n",
    "        gleason = self.cancer_df.iloc[idx, 3]\n",
    "        # Recommend using downsample rate of 16 to speed up resizing\n",
    "        sample = {'image': img[-1], 'isup_grade': isup, 'gleason_score': gleason}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize Transforms\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image sample to the given size\n",
    "    Args:\n",
    "        output_size (tuple): Desired output size. Output is matched to output_size.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, tuple)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img = transform.resize(sample['image'], self.output_size)\n",
    "        return {'image': img, 'isup_grade': sample['isup_grade'], 'gleason_score': sample['gleason_score']}\n",
    "    \n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']        \n",
    "        # Swap color axis to [C,H,W]\n",
    "        img = img.transpose(2,0,1)\n",
    "        return {'image': img, 'isup_grade': sample['isup_grade'], 'gleason_score': sample['gleason_score']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_cancer = ProstateCancerDataset(csv_file='train_512.csv',\n",
    "                                     root_dir=image_dir,\n",
    "                                     transform=Rescale((512, 512)))\n",
    "\n",
    "print(len(train_cancer))\n",
    "for i in range(len(train_cancer)):\n",
    "    sample = train_cancer[i]\n",
    "    print(i, sample['image'].shape, sample['isup_grade'], sample['gleason_score'])\n",
    "    if i == 2: \n",
    "        plt.imshow(sample['image'])\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compse tranforms of rescale and totensor\n",
    "# More transformer to try: crop, normalize, etc.\n",
    "# And transformer is a useful tool for data augmentation\n",
    "biopsy_train = ProstateCancerDataset(csv_file='train_512.csv',\n",
    "                                     root_dir=image_dir,\n",
    "                                     transform=T.Compose([\n",
    "                                         Rescale((512, 512)),\n",
    "                                         ToTensor()\n",
    "                                     ]))\n",
    "\n",
    "loader_train = DataLoader(biopsy_train, batch_size=8, num_workers=4,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "loader_val = DataLoader(biopsy_train, batch_size=4, num_workers=4,\n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, NUM_TRAIN+100)))  #8492  Use smaller size for debug"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for batch_i, batch_sample in enumerate(loader_val):\n",
    "    #print(batch_sample['image'][0].shape)\n",
    "    #for s_i, sample in enumerate(batch_sample['isup_grade']):\n",
    "    #    print(sample)\n",
    "    print(batch_sample['isup_grade'])\n",
    "    #plt.imshow(batch_sample['image'][0].permute(1,2,0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch['image'].to(device=device, dtype=dtype)\n",
    "            y = batch['isup_grade'].to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            #print(scores.max(1))\n",
    "            #assert False\n",
    "            num_correct += (preds==y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got {:d}/{:d} correct {:.2f}'.format(num_correct, num_samples, acc*100))\n",
    "\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)\n",
    "\n",
    "def train_sequential(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model using PyTorch Sequential API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model.\n",
    "    - epochs: The expected usage number of each image.\n",
    "    \n",
    "    Output: Print model accuracies.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)\n",
    "    for e in range(epochs):\n",
    "        for t, batch in enumerate(loader_train):\n",
    "            x = batch['image'].to(device=device, dtype=dtype)\n",
    "            y = batch['isup_grade'].to(device=device, dtype=torch.long)\n",
    "            \n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            \n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass: compute the gradient of the loss with respect to\n",
    "            # each parameter of the model.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters of the model using the gradients computed by\n",
    "            # the backward pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Iteration {:d}, loss = {:.4f}'.format(t, loss.item()))\n",
    "                check_accuracy(loader_val, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1.8226\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 10, loss = 1.7507\n",
      "Got 13/100 correct 13.00\n",
      "Iteration 20, loss = 1.7715\n",
      "Got 13/100 correct 13.00\n",
      "Iteration 30, loss = 1.7680\n",
      "Got 13/100 correct 13.00\n",
      "Iteration 40, loss = 1.8237\n",
      "Got 13/100 correct 13.00\n",
      "Iteration 50, loss = 1.8036\n",
      "Got 13/100 correct 13.00\n",
      "Iteration 60, loss = 1.7774\n",
      "Got 13/100 correct 13.00\n",
      "Iteration 70, loss = 1.7950\n",
      "Got 13/100 correct 13.00\n",
      "Iteration 80, loss = 1.7736\n",
      "Got 13/100 correct 13.00\n",
      "Iteration 90, loss = 1.7442\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 100, loss = 1.8084\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 110, loss = 1.7659\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 120, loss = 1.7239\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 130, loss = 1.7792\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 140, loss = 1.7925\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 150, loss = 1.7965\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 160, loss = 1.6866\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 170, loss = 1.7905\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 180, loss = 1.6924\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 190, loss = 1.7596\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 200, loss = 1.8709\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 210, loss = 1.7234\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 220, loss = 1.7921\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 230, loss = 1.7552\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 240, loss = 1.7161\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 250, loss = 1.7836\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 260, loss = 1.7801\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 270, loss = 1.8045\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 280, loss = 1.6785\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 290, loss = 1.7597\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 300, loss = 1.7574\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 310, loss = 1.8095\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 320, loss = 1.7741\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 330, loss = 1.7588\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 340, loss = 1.9333\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 350, loss = 1.7046\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 360, loss = 1.7370\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 370, loss = 1.8576\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 380, loss = 1.8566\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 390, loss = 1.6324\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 400, loss = 1.6983\n",
      "Got 32/100 correct 32.00\n",
      "Iteration 410, loss = 1.6561\n"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "    \n",
    "hidden_layer_size = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3*512*512, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 6)\n",
    ")\n",
    "\n",
    "# Use Nesterov momentum\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=learning_rate,\n",
    "                      momentum=.9,\n",
    "                      nesterov=True)\n",
    "\n",
    "train_sequential(model, optimizer, 2)"
   ]
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "MarkdownCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
